{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"fhv_tripdata_2021-06.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 2, 41), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 7, 46), PULocationID=174, DOLocationID=18, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 16, 16), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 21, 14), PULocationID=32, DOLocationID=254, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 27, 1), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 42, 11), PULocationID=240, DOLocationID=127, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 46, 8), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 53, 45), PULocationID=127, DOLocationID=235, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 45, 42), dropoff_datetime=datetime.datetime(2021, 6, 1, 1, 3, 33), PULocationID=144, DOLocationID=146, SR_Flag='N', Affiliated_base_number=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Repartition it to 12 partitions and save it to parquet\n",
    "\n",
    "df.repartition(12).write.parquet(\"fhv_tripdata_2021-06.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 22.421844879786175 MB\n"
     ]
    }
   ],
   "source": [
    "### Average size of the parquet files in MB\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "parquet_files = glob.glob(\"fhv_tripdata_2021-06.parquet/*.parquet\")\n",
    "parquet_files_size = [os.path.getsize(file) for file in parquet_files]\n",
    "a = sum(parquet_files_size) / len(parquet_files_size) / 1024 / 1024\n",
    "\n",
    "print('Answer:', a, 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read.parquet('fhv_tripdata_2021-06.parquet/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02875|2021-06-03 16:28:28|2021-06-03 17:01:11|          15|         145|      N|                B02875|\n",
      "|              B02887|2021-06-01 09:41:03|2021-06-01 10:02:55|          95|         170|      N|                B02887|\n",
      "|              B02882|2021-06-03 16:07:59|2021-06-03 16:43:18|         161|          87|      N|                B02882|\n",
      "|              B02889|2021-06-03 23:33:49|2021-06-03 23:39:09|          21|          21|      N|                B02889|\n",
      "|              B02867|2021-06-02 18:20:05|2021-06-02 21:33:56|          92|         265|      N|                B02867|\n",
      "|              B02887|2021-06-01 16:37:26|2021-06-01 17:05:24|         177|         132|      N|                B02887|\n",
      "|              B02510|2021-06-03 15:28:46|2021-06-03 15:40:23|         140|         229|      N|                  null|\n",
      "|              B02887|2021-06-03 11:20:15|2021-06-03 11:39:16|          64|         171|      N|                B02887|\n",
      "|              B02510|2021-06-01 00:48:20|2021-06-01 00:55:51|         243|         127|      N|                  null|\n",
      "|              B02765|2021-06-03 01:10:01|2021-06-03 01:18:28|          87|         224|      N|                B02765|\n",
      "|              B02879|2021-06-04 19:17:12|2021-06-04 19:53:02|         249|         223|      N|                B02879|\n",
      "|              B02875|2021-06-01 19:50:50|2021-06-01 20:08:04|         246|         116|      N|                B02875|\n",
      "|              B02879|2021-06-01 21:48:05|2021-06-01 22:10:00|          65|          28|      N|                B02879|\n",
      "|              B02510|2021-06-01 22:59:51|2021-06-01 23:16:39|         100|         265|      N|                  null|\n",
      "|              B02870|2021-06-03 16:14:52|2021-06-03 16:22:27|         250|         242|      N|                B02870|\n",
      "|              B02888|2021-06-01 12:32:47|2021-06-01 12:41:23|         161|         100|      N|                B02888|\n",
      "|              B02869|2021-06-02 11:09:35|2021-06-02 11:42:49|         256|         181|      N|                B02869|\n",
      "|              B02510|2021-06-03 22:23:56|2021-06-03 22:26:17|         225|         225|      N|                  null|\n",
      "|              B02510|2021-06-02 07:10:31|2021-06-02 07:23:39|         234|          13|      N|                  null|\n",
      "|              B02872|2021-06-02 11:03:44|2021-06-02 11:16:36|         159|         119|      N|                B02872|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\data_engineering_camp\\lib\\site-packages\\pyspark\\sql\\dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_fhv.registerTempTable('fhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_june_15 = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    count(*)\n",
    "FROM fhv \n",
    "WHERE pickup_datetime BETWEEN '2021-06-15 00:00:00' AND '2021-06-15 23:59:59'\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv_june_15.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_longest_trip = spark.sql (\"\"\"\n",
    "SELECT CAST(pickup_datetime AS DATE) AS FECHA,\n",
    "    MAX(unix_timestamp(dropoff_datetime)- unix_timestamp(pickup_datetime))/3600\n",
    "FROM fhv\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC \n",
    "LIMIT 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|     FECHA|(max((unix_timestamp(dropoff_datetime, yyyy-MM-dd HH:mm:ss) - unix_timestamp(pickup_datetime, yyyy-MM-dd HH:mm:ss))) / 3600)|\n",
      "+----------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|2021-06-25|                                                                                                            66.8788888888889|\n",
      "+----------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv_longest_trip.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Answer: 4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi_zone_lookup = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"taxi_zone_lookup.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_zone_lookup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\data_engineering_camp\\lib\\site-packages\\pyspark\\sql\\dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_taxi_zone_lookup.registerTempTable('taxi_zone_lookup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\" \n",
    "SELECT COUNT(pul_zone.Zone), pul_zone.Zone\n",
    "FROM fhv \n",
    "JOIN taxi_zone_lookup pul_zone ON fhv.PULocationID = pul_zone.LocationID\n",
    "JOIN taxi_zone_lookup dol_zone ON fhv.DOLocationID = dol_zone.LocationID\n",
    "GROUP BY pul_zone.Zone\n",
    "ORDER BY 1 DESC \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|count(Zone)|                Zone|\n",
      "+-----------+--------------------+\n",
      "|     231279| Crown Heights North|\n",
      "|     221244|        East Village|\n",
      "|     188867|         JFK Airport|\n",
      "|     187929|      Bushwick South|\n",
      "|     186780|       East New York|\n",
      "|     164344|TriBeCa/Civic Center|\n",
      "|     161596|   LaGuardia Airport|\n",
      "|     158937|            Union Sq|\n",
      "|     154698|        West Village|\n",
      "|     152493|             Astoria|\n",
      "|     151020|     Lower East Side|\n",
      "|     147673|        East Chelsea|\n",
      "|     146402|Central Harlem North|\n",
      "|     143683|Williamsburg (Nor...|\n",
      "|     143594|          Park Slope|\n",
      "|     141427|  Stuyvesant Heights|\n",
      "|     139611|        Clinton East|\n",
      "|     139431|West Chelsea/Huds...|\n",
      "|     138428|             Bedford|\n",
      "|     137879|         Murray Hill|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_engineering_camp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
